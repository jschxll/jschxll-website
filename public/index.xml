<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Jan's site</title><link>http://localhost:1313/</link><description>Recent content in Home on Jan's site</description><generator>Hugo</generator><language>en-US</language><copyright>Copyright © 2024, Jan Schall.</copyright><lastBuildDate>Mon, 21 Oct 2024 10:51:10 +0200</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml"/><item><title>Apx</title><link>http://localhost:1313/photos/apx/</link><pubDate>Mon, 21 Oct 2024 10:51:10 +0200</pubDate><guid>http://localhost:1313/photos/apx/</guid><description/></item><item><title>Waning Moon 22.09.2024</title><link>http://localhost:1313/photos/waning-moon-22.09.2024/</link><pubDate>Sun, 22 Sep 2024 20:21:50 +0200</pubDate><guid>http://localhost:1313/photos/waning-moon-22.09.2024/</guid><description/></item><item><title>Terms of Privacy</title><link>http://localhost:1313/terms-of-privacy/</link><pubDate>Tue, 13 Aug 2024 19:22:07 +0200</pubDate><guid>http://localhost:1313/terms-of-privacy/</guid><description>Terms of Privacy 1. Introduction The operator of this website is committed to protecting your data. This privacy policy informs you about the nature, scope, and purpose of the collection and use of personal data on this website.
2. Responsible Party The responsible party in accordance with data protection laws is:
Jens Schall jschxll@proton.me
3. Data Collection and Processing This website does not collect personal data unless you voluntarily provide it (e.</description></item><item><title>Smarthome plasmaball</title><link>http://localhost:1313/posts/smarthome-plasmaball/</link><pubDate>Mon, 12 Aug 2024 10:34:13 +0200</pubDate><guid>http://localhost:1313/posts/smarthome-plasmaball/</guid><description>So, yeah, I guess I don&amp;rsquo;t need to add much more than the title already reveals: I built a smarthome integration for my plasma ball. Why? I don&amp;rsquo;t know, I thought it would be a fun idea, but in the end it got much worse than I thought, because I don&amp;rsquo;t have really a good practice in soldering and electrical engineering. Good conditions, to start such a project, I think.</description></item><item><title>Legal Notice</title><link>http://localhost:1313/legal-notice/</link><pubDate>Sun, 11 Aug 2024 20:52:53 +0200</pubDate><guid>http://localhost:1313/legal-notice/</guid><description>The entire content of this website, including articles and images, is licensedunderCC BY-NC 4.0with the exception of content identified as resources from other sources.Say hello (。・∀・)ノ jschxll@proton.me</description></item><item><title>Slieve League Ireland</title><link>http://localhost:1313/photos/slieve-league-ireland/</link><pubDate>Mon, 05 Aug 2024 12:49:12 +0200</pubDate><guid>http://localhost:1313/photos/slieve-league-ireland/</guid><description/></item><item><title>Glencar Waterfall Ireland</title><link>http://localhost:1313/photos/glencar-waterfall-ireland/</link><pubDate>Thu, 01 Aug 2024 11:42:19 +0200</pubDate><guid>http://localhost:1313/photos/glencar-waterfall-ireland/</guid><description/></item><item><title>My Arch Linux experience</title><link>http://localhost:1313/posts/arch-linux-experience/</link><pubDate>Tue, 23 Jul 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/arch-linux-experience/</guid><description>So I had an old ThinkPad T510 lying around in my room and I wondered what I could do with it: The BIOS was last updated in 2012 and the preinstalled Windows 10 ran on a 2,5&amp;quot; HDD with a capacity of 160GB. As expected, the Windows experience was terrible, especially when considering the whole machine runs with 4GB of RAM. First, it was clear I had to replace the HDD with a decent 2,5&amp;quot; SATA SSD.</description></item><item><title>Run LLMs locally</title><link>http://localhost:1313/posts/ollama_web_ui/</link><pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/ollama_web_ui/</guid><description>How to Run Open-Source LLMs on a Local Computer Without Compromising on a Specific Use Case? Today, I will show you how to run LLMs locally on a computer or home server using Ollama in combination with the OpenUI web interface. What is Ollama? First, Ollama is an engine that allows you to run Large Language Models (LLMs) on your own computer, so no personal data from sessions, like those in ChatGPT, is sent to companies.</description></item><item><title>Building an own cloud</title><link>http://localhost:1313/posts/buidling-an-own-cloud/</link><pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/buidling-an-own-cloud/</guid><description>In recent weeks, I have been using my own cloud to back up my photos and videos from my smartphone and laptop by using a containerized Photoprism instance. Today, I want to share how I set up my server and Docker environment to get everything running.
Installing the Server&amp;rsquo;s Operating System First, you should use a separate computer so your own cloud is always accessible when you need to access your photos.</description></item></channel></rss>