<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ollama on Jan's site</title><link>https://jschall.net/posts/ollama/</link><description>Recent content in Ollama on Jan's site</description><generator>Hugo</generator><language>en-US</language><copyright>Copyright Â© 2024, Jan Schall.</copyright><lastBuildDate>Sun, 05 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jschall.net/posts/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>Run LLMs locally</title><link>https://jschall.net/posts/ollama_web_ui/</link><pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate><guid>https://jschall.net/posts/ollama_web_ui/</guid><description>How to Run Open-Source LLMs on a Local Computer Without Compromising on a Specific Use Case? Today, I will show you how to run LLMs locally on a computer or home server using Ollama in combination with the OpenUI web interface. What is Ollama? First, Ollama is an engine that allows you to run Large Language Models (LLMs) on your own computer, so no personal data from sessions, like those in ChatGPT, is sent to companies.</description></item></channel></rss>