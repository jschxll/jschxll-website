<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Jan's site</title><link>http://localhost:1313/posts/</link><description>Recent content in Posts on Jan's site</description><generator>Hugo</generator><language>en-US</language><copyright>Copyright Â© 2024, Jan Schall.</copyright><lastBuildDate>Mon, 12 Aug 2024 10:34:13 +0200</lastBuildDate><atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Smarthome plasmaball</title><link>http://localhost:1313/posts/smarthome-plasmaball/</link><pubDate>Mon, 12 Aug 2024 10:34:13 +0200</pubDate><guid>http://localhost:1313/posts/smarthome-plasmaball/</guid><description>So, yeah, I guess I don&amp;rsquo;t need to add much more than the title already reveals: I built a smarthome integration for my plasma ball. Why? I don&amp;rsquo;t know, I thought it would be a fun idea, but in the end it got much worse than I thought, because I don&amp;rsquo;t have really a good practice in soldering and electrical engineering. Good conditions, to start such a project, I think.</description></item><item><title>My Arch Linux experience</title><link>http://localhost:1313/posts/arch-linux-experience/</link><pubDate>Tue, 23 Jul 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/arch-linux-experience/</guid><description>So I had an old ThinkPad T510 lying around in my room and I wondered what I could do with it: The BIOS was last updated in 2012 and the preinstalled Windows 10 ran on a 2,5&amp;quot; HDD with a capacity of 160GB. As expected, the Windows experience was terrible, especially when considering the whole machine runs with 4GB of RAM. First, it was clear I had to replace the HDD with a decent 2,5&amp;quot; SATA SSD.</description></item><item><title>Run LLMs locally</title><link>http://localhost:1313/posts/ollama_web_ui/</link><pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/ollama_web_ui/</guid><description>How to Run Open-Source LLMs on a Local Computer Without Compromising on a Specific Use Case? Today, I will show you how to run LLMs locally on a computer or home server using Ollama in combination with the OpenUI web interface. What is Ollama? First, Ollama is an engine that allows you to run Large Language Models (LLMs) on your own computer, so no personal data from sessions, like those in ChatGPT, is sent to companies.</description></item><item><title>Building an own cloud</title><link>http://localhost:1313/posts/buidling-an-own-cloud/</link><pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/buidling-an-own-cloud/</guid><description>In recent weeks, I have been using my own cloud to back up my photos and videos from my smartphone and laptop by using a containerized Photoprism instance. Today, I want to share how I set up my server and Docker environment to get everything running.
Installing the Server&amp;rsquo;s Operating System First, you should use a separate computer so your own cloud is always accessible when you need to access your photos.</description></item></channel></rss>