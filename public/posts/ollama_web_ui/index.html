<!doctype html><html lang=en-US><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=http://localhost:1313/images/favicon.png><title>Run LLMs locally | Jan's site</title>
<meta name=title content="Run LLMs locally"><meta name=description content="Running open source LLMs on you local machine isn't anymore a dream. With Ollama and the OpenUI, you can run and modify LLMs for your needs and preferences in a decent web interface."><meta name=keywords content="linux,homeserver,docker,ollama,"><meta property="og:url" content="http://localhost:1313/posts/ollama_web_ui/"><meta property="og:site_name" content="Jan's site"><meta property="og:title" content="Run LLMs locally"><meta property="og:description" content="Running open source LLMs on you local machine isn't anymore a dream. With Ollama and the OpenUI, you can run and modify LLMs for your needs and preferences in a decent web interface."><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-05T00:00:00+00:00"><meta property="article:tag" content="Linux"><meta property="article:tag" content="Homeserver"><meta property="article:tag" content="Docker"><meta property="article:tag" content="Ollama"><meta property="og:image" content="http://localhost:1313/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/share.png"><meta name=twitter:title content="Run LLMs locally"><meta name=twitter:description content="Running open source LLMs on you local machine isn't anymore a dream. With Ollama and the OpenUI, you can run and modify LLMs for your needs and preferences in a decent web interface."><meta itemprop=name content="Run LLMs locally"><meta itemprop=description content="Running open source LLMs on you local machine isn't anymore a dream. With Ollama and the OpenUI, you can run and modify LLMs for your needs and preferences in a decent web interface."><meta itemprop=datePublished content="2024-05-05T00:00:00+00:00"><meta itemprop=dateModified content="2024-05-05T00:00:00+00:00"><meta itemprop=wordCount content="548"><meta itemprop=image content="http://localhost:1313/images/share.png"><meta itemprop=keywords content="Linux,Homeserver,Docker,Ollama"><meta name=referrer content="no-referrer-when-downgrade"><style>@font-face{font-family:SpaceGrotesk;src:url(../../fonts/SpaceGrotesk-Regular.ttf)}@font-face{font-family:FiraMono;src:url(../../fonts/FiraMono-Regular.ttf)}@font-face{font-family:Lora;src:url(../../fonts/Lora-Regular.ttf)}body{font-family:FiraMono;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#1d1d1d;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#e3dddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#b39ddb}.title{text-decoration:none;border:0}.title span{font-weight:400}nav{display:flex;justify-content:space-between;font-size:medium;font-family:SpaceGrotesk}nav:last-child{color:#817373}nav a{color:#d65d0e;margin-right:10px;text-decoration:underline 3px;font-weight:700;font-size:1.2em}textarea{width:100%;font-size:16px;background-color:#252525;color:#ddd}input{font-size:16px;background-color:#252525;color:#ddd}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#777}pre code{color:#ddd;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}#night-sky{max-width:100%;display:flex;justify-content:center}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#ccc;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#aaa;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}.post-data{display:flex;justify-content:space-between}@media only screen and (max-width:600px){.post-data{display:flex;flex-direction:column;align-items:start}nav{display:block}}</style><link rel=authorization_endpoint href=https://indieauth.com/auth><link rel=token_endpoint href=https://tokens.indieauth.com/token></head><body><header><nav><div><a href=/>home</a>
<a href=/posts>posts</a>
<a href=/photos>photos</a>
<a href=https://github.com/jschxll rel=me>git</a></div><div><span>/posts/ollama_web_ui</span></div></nav></header><main><head><style>body{font-family:Lora}h1,.post-data{font-family:FiraMono}h2,h3,h4,h5,h6{font-style:italic}</style></head><h1>Run LLMs locally</h1><div class=post-data><div><time datetime=2024-05-05 pubdate>05 May, 2024</time></div><div><a href=http://localhost:1313/posts/linux/>#Linux</a>
<a href=http://localhost:1313/posts/homeserver/>#Homeserver</a>
<a href=http://localhost:1313/posts/docker/>#Docker</a>
<a href=http://localhost:1313/posts/ollama/>#Ollama</a></div></div><p></p><content class=h-card><p>How to Run Open-Source LLMs on a Local Computer Without Compromising on a Specific Use Case? Today, I will show you how to run LLMs locally on a computer or home server using Ollama in combination with the OpenUI web interface.</p><h2 id=what-is-ollama>What is Ollama?</h2><p>First, Ollama is an engine that allows you to run Large Language Models (LLMs) on your own computer, so no personal data from sessions, like those in ChatGPT, is sent to companies. Instead, the data stays on the PC and runs on your own hardware. You can download officially supported LLMs from Microsoft, Mistral, and even Meta, which you can customize with model files to suit your needs. With the Ollama WebUI, features like RAG (Retrieval-Augmented Generation) are directly integrated, allowing you to upload your documents, such as PDFs, to provide a new data source for the AI to consider in its next response. Additionally, websites and images can be inserted into a prompt so you can formulate your questions on current websites and multimodal LLMs like Llava can recognize images.</p><h3 id=requirements>Requirements</h3><p>Although Ollama can also run on a CPU, I would recommend a good GPU. I run all my Large Language Models on an NVIDIA RTX 3060, but an AMD graphics card is also a good choice.</p><h2 id=installation>Installation</h2><p>Visit the Ollama website and click on Download. Linux and macOS are fully supported, but the Windows preview is also usable (I use this myself).
Open the terminal, paste the command for the installation script, and run it. That&rsquo;s it! Now we want to download the web interface to have a better experience and more features. For this, you need to have Docker installed. If Docker is not installed on your PC, follow the steps on the official Docker website.
Finally, we will install Ollama WebUI. Enter this command in the terminal: <code>sudo docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</code> and press ENTER. Now the Open WebUI image should be pulled from the Docker Hub. To check whether the container is running as expected, copy the command into your terminal: sudo docker ps. Now open localhost and create a new Ollama WebUI account. This step is important because different users can be created in the local instance and the first user has administrator rights.</p><h2 id=quick-overview-of-ollama-webui>Quick Overview of Ollama WebUI</h2><p>Once everything is set up, it&rsquo;s time to install the first LLM on your computer. To get an idea of which model you want to run, there is an overview of every available LLM on the Ollama website. To install a model, run this command: ollama run followed by the name of the model. Once Ollama has downloaded the model, refresh the Ollama WebUI, where the model will now be displayed. Click on the dropdown menu and select the model you want to chat with.</p><h3 id=adding-websites-and-documents-to-prompts>Adding Websites and Documents to Prompts</h3><p>As mentioned earlier, websites and documents can be added to prompts. To do this, type ‚Äú#‚Äù followed by the corresponding web address or document you want the LLM to consider. Documents can also be added by pressing the ‚Äú+‚Äù button next to the text field.
Now that you understand the basics of Ollama and the web interface, it&rsquo;s time to explore the possibilities with local LLMs and Ollama WebUI.</p></content><section class=post-end><a class=prev href=http://localhost:1313/posts/buidling-an-own-cloud/>Prev &raquo</a>
<a class=next href=http://localhost:1313/posts/arch-linux-experience/>&laquo Next</a></section><style>.prev{float:right}.next{float:left}.post-end{margin-top:40px;margin-bottom:40px}</style></main><footer><span>Made with</span> <a class=footer-link href=https://github.com/janraasch/hugo-bearblog/>Hugo  ï‚Ä¢·¥•‚Ä¢ î Bear</a>
|
<a class=footer-link href=/index.xml>RSS</a>
|
<a class=footer-link href=/legal-notice>legal notice</a>
|
<a class=footer-link href=/terms-of-privacy>terms of privacy</a><br><a class=footer-link href=https://xn--sr8hvo.ws/previous>‚Üê</a>
<span>An</span> <a class=footer-link href=https://xn--sr8hvo.ws>IndieWeb Webring</a> üï∏üíç
<a class=footer-link href=https://xn--sr8hvo.ws/next>‚Üí</a><style>.footer-link{font-size:small}span{font-size:small}</style></footer></body></html>